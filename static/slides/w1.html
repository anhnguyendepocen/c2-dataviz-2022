<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Welcome!</title>
    <meta charset="utf-8" />
    <meta name="author" content="Daniel Anderson" />
    <script src="libs/header-attrs-2.11/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <script src="libs/kePrint-0.0.1/kePrint.js"></script>
    <link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
    <script src="https://unpkg.com/feather-icons"></script>
    <link rel="stylesheet" href="new.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Welcome!
## An overview of the course
### Daniel Anderson
### Week 1, Class 1

---




layout: true

  &lt;script&gt;
    feather.replace()
  &lt;/script&gt;
  
  &lt;div class="slides-footer"&gt;
  &lt;span&gt;
  
  &lt;a class = "footer-icon-link" href = "https://github.com/uo-datasci-specialization/c2-dataviz-2022/raw/main/static/slides/w1p1.pdf"&gt;
    &lt;i class = "footer-icon" data-feather="download"&gt;&lt;/i&gt;
  &lt;/a&gt;
  
  &lt;a class = "footer-icon-link" href = "https://dataviz-2022.netlify.app/slides/w1p1.html"&gt;
    &lt;i class = "footer-icon" data-feather="link"&gt;&lt;/i&gt;
  &lt;/a&gt;
  
  &lt;a class = "footer-icon-link" href = "https://github.com/uo-datasci-specialization/c2-dataviz-2022"&gt;
    &lt;i class = "footer-icon" data-feather="github"&gt;&lt;/i&gt;
  &lt;/a&gt;
  
  &lt;/span&gt;
  &lt;/div&gt;
  

---
# Agenda 

.pull-left[

* Getting on the same page
* Syllabus
* Accessing and working with the course data
* If time allows - Intro to text data
]

.pull-right[
![](../edld652-logo.png)
]

---

# whoami

.pull-left[
* Research Associate Professor: Behavioral Research and Teaching
* Dad (two daughters: 9 and 7)
* Pronouns: he/him/his
* Primary areas of interest: 💗💗R💗💗, computational research, systemic inequities in opportunities and achievement, and variance between educational institutions
]

.pull-right[

![](img/IMG_1306.jpeg)
&lt;/div&gt;
]

---
class: center middle inverse-blue

# whoisyou?

.left[
* Introduce yourself
* Why are you here?
* What pronouns would you like us to use for you for this class?
* What was one thing you did not related to academic work over winter break?
]

---

# A few class policies

--
* Be kind


--
* Be understanding and have patience, with others **and yourself**


--
* Help others whenever possible


--
Truly the most important part of this class. Important not just in terms of decency, but also in your learning, and most importantly, for equity. 

---
# A more specific policy
### Kiddos in class

--

* All breastfeeding babies are welcome in class as often as necessary.

--

* Non-nursing babies and older children are welcome whenever alternate arrangements cannot be made. As a parent of two young children, I understand that babysitters fall through, partners have conflicting schedules, children get sick, and other issues arise that leave parents with few other options.

---

* In cases where children come to class, I invite parents/caregivers to sit close to the door so as to more easily excuse yourself to attend to your child's needs. Non-parents in the class: please reserve seats near the door for your parenting classmates.

--

* All students are expected to join with me in creating a welcoming environment that is respectful of your classmates who bring children to class.

---
# Last intro thing

* I'm here for you

* We won't have specific office hours, but know I'm always willing to meet

* This course, like all in the sequence, can be difficult. Don't suffer in silence. Don't do this alone.

---
class: inverse-green middle
background-size:cover

# Syllabus

---

# Course Website(s)

.pull-left[
## [website](https://dataviz-2022.netlify.app)
]

.pull-right[
.right[
## [repo](https://github.com/uo-datasci-specialization/c2-dataviz-2022)
  ]
]


&lt;iframe src="https://dataviz-2022.netlify.app" width="100%" height="400px" data-external="1"&gt;&lt;/iframe&gt;

---
# Materials
* Nearly everything will be distributed through the repo and through the website.

* Please clone the repo now, if you haven't already. Pull each week for the most recent changes.

* We'll use Canvas for grading, and that is essentially it.

---
# R Markdown notes
* These slides were produced with [**{xaringan}**](https://github.com/yihui/xaringan), an R Markdown variant. I encourage you to try it out and use it for your final project presentation.

* The website was also produced with R Markdown (sort of)
  + It's a [**{blogdown}**](https://github.com/rstudio/blogdown) website with some custom CSS and Hugo shortcodes
  
* This course is not just about data viz, but also mediums for communication. This includes websites and [data dashboards](https://jenthompson.me/examples/insight_progress.html) among other possibilities.


---
class: inverse-red middle

# My assumptions about you

---
# I assume you

* Understand the R package ecosystem (how to find, install, load, and learn about them)


--
* Can read "flat" (i.e., rectangular) datasets into R 

  + I don't care what you use, but you should be using RStudio Projects &amp; the [{here}](https://github.com/r-lib/here) package

    - See [Jenny Bryan's blog post](https://www.tidyverse.org/articles/2017/12/workflow-vs-script/) for why.



---
* Can perform basic data wrangling and transformations in R, using the
tidyverse  

  + Leverage appropriate functions for introductory data science tasks (pipeline)
  + "clean up" the dataset using scripts and reproducible workflows

--

* Use version control with R via git and GitHub  


--
* Use R Markdown to create reproducible dynamic reports


---
# Learning objectives

* Transform data in a variety of ways to create effective data visualizations

--

* Understand and and be able to apply basic string operations and work with textual data


--
* Understand best practices in data visualization


--

* Customize ggplot2 graphics by reordering factors, creating themes, etc.


--

* Create an online data visualization portfolio using distill and/or flexdashboards to demonstrate key learning 


---

# Examples
Below are some links to final projects from students who have taken this class previously.

.pull-left[
### Dashboards
* [Alexis Adams-Clark](https://alexisadamsclark.github.io/dashboard_finalproj/)
* [Brendan Cullen](https://brendanhcullen.github.io/data-viz-dashboard/)
* [Maggie Osa](https://maggieosa.shinyapps.io/652finalproj/)
]


.pull-right[
### Blog post
* [Teresa Chen](https://teresashchen.github.io/blog/)
* [Ouafaa Hmaddi](https://ohmaddi.github.io/Portfolio-Kiva/)
* [Murat Kezer](https://mkezer.github.io/Moral-values-across-countries/#predicted-values-of-moral-values-by-gender-equality)
]

---
# Weekly learning objectives

Provide you a frame for what you should be working to learn for that specific week.

--

### This week's objectives
* Understand the requirements of the course
* Understand the requirements of the final project
* Be ready to go with *git* and GitHub
* Understand how to access the course data and documentation, begin playing with the data


---

# Required Textbooks (free)

.pull-left[
## [Healy](http://socviz.co)

&lt;div&gt;
&lt;img src = http://socviz.co/assets/dv-cover-pupress.jpg height = 400&gt;
&lt;/div&gt;

]

.pull-right[
.right[
## [Wilke](https://serialmentor.com/dataviz/)
]

&lt;div&gt;
&lt;img src =
https://clauswilke.com/dataviz/cover.png height = 400&gt; &lt;/div&gt;


]


---

# Other books (also free)

.pull-left[
![](https://happygitwithr.com/img/watch-me-diff-watch-me-rebase-smaller.png)
## [Bryan](http://happygitwithr.com)
]

.pull-right[
.right[
&lt;div&gt;
&lt;img src =https://d33wubrfki0l68.cloudfront.net/b88ef926a004b0fce72b2526b0b5c4413666a4cb/24a30/cover.png height = 400&gt; 
&lt;/div&gt;
  ]
]

.right[
### [Wickham &amp; Grolemund](https://r4ds.had.co.nz)
]
---
class: inverse-green middle

## Another resource
See the current draft [here](https://www.sds.pub). Please read Chapter 8 on collaborating with git/GitHub. There is also a video lecture on this topic from last year that is linked on the website.

&lt;iframe src="https://www.sds.pub" width="100%" height="400px" data-external="1"&gt;&lt;/iframe&gt;


---
class: inverse-orange middle
# Extra credit opportunity
**5 points**: Deep dive into a topic not covered by the course


---
# Some options

* Geographic data (we will discuss this, but it's relatively late and there's a ton we won't be able to get to)
* Network data
* DAGs
* Flow data (e.g., alluvial diagrams)
* Interactive plots
* Animated plots

---
class: inverse-blue  middle
# Some examples

---
background-image:url(https://timogrossenbacher.ch/wp-content/uploads/2016/12/tm-final-map-1.png)
background-size:contain

&lt;br/&gt;


[Timo Grossenbacher](https://timogrossenbacher.ch/2016/12/beautiful-thematic-maps-with-ggplot2-only/)

---
class: inverse
background-image:url(https://user-images.githubusercontent.com/25231784/41408567-08313b66-6fcb-11e8-8c55-75baa36364cd.png)
background-size:contain

&lt;br/&gt;
&lt;br/&gt;

[Paul Campbell](https://gist.github.com/PaulC91/e767ca4f0c4335e6e0d2f71eb7cc98cc)

---
class: bottom
background-image:url(https://ggdag.netlify.com/articles/intro-to-dags_files/figure-html/unnamed-chunk-11-1.png)
background-size:contain

&lt;br/&gt;
&lt;br/&gt;

[ggdag](https://ggdag.netlify.com/articles/intro-to-dags.html) via Malcolm Barrett


---
class: bottom
background-image:url(https://static01.nyt.com/images/2018/05/02/learning/economic-mobilityLN/economic-mobilityLN-superJumbo.png?quality=90&amp;auto=webp)
background-size:contain

&lt;br/&gt;
&lt;br/&gt;

[Patrick Honner](https://www.nytimes.com/2018/05/03/learning/lesson-plans/moving-on-up-teaching-with-the-data-of-economic-mobility.html) via NYT

---
class: bottom
background-image:url(https://cloud.githubusercontent.com/assets/7896861/17839509/d66b3c2a-67b7-11e6-9ee4-5f8ad54746d7.gif)
background-size:contain

&lt;br/&gt;
&lt;br/&gt;

[James Curley](https://github.com/jalapic/nba)

---
# Labs

See the [assignments](https://dataviz-2022.netlify.app/assignments/) page of the website.

15 points each (45 points total; 30%)

1. Distributions, GitHub collabo, and working w/strings

1. Visual perception  &amp; plot reproducing

1. Color


---
# Homework
Only one this time, worth 30 points (20%)

* Basically the same as the labs, but scored correct/incorrect, and no in-class time devoted to them.

* Okay to work on collaboratively - I actively encourage you to do so as long as you're using a shared repo


--

## Topic: Visualizing uncertainty, tables, and plot refinement


---
# Data viz "in the wild" presentations
Everyone will be randomly assigned a date to share two data visualizations you have found in publications, websites, or anywhere else IRL.

* Not a formal presentation

* Share the links with me before class - we'll look at it as a group and discuss

* You note where you found it and what you like/dislike about it

---

# Presentation order

.footnote[I will email this out as well. ]



.pull-left[
&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; Date &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Presenter &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 2022-01-10 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Futing &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 2022-01-10 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Zach &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 2022-01-10 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Cano &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 2022-01-17 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Ian &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 2022-01-17 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Abbie &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 2022-01-24 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Havisha &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 2022-01-24 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Tingyu &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 2022-01-31 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Dillon &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 2022-01-31 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Eliott &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 2022-02-07 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Merly &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 2022-02-07 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Esmeralda &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
]

.pull-right[
&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; Date &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Presenter &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 2022-02-14 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Amy &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 2022-02-14 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Diana &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 2022-02-21 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Errol &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 2022-02-21 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Mandi &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 2022-02-28 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Adriana &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 2022-02-28 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Rebecca &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
]

---
class: inverse-red middle
# Final Project 
70 points total (46.66%)

---
# Group project

* Please try to finalize your group by the end of today. You will have time when exploring the course data to work together.

* No fewer than 2, no more than 3.

* Although the final is the only mandated group project, I encourage you to work with your group for all labs and the homework assignment as well.

---
# Five parts
* Proposal (5 points): Due 1/24/22

* Draft (10 points): Due 2/21/22

* Peer review (10 points): Assigned, 2/21/21; Due 2/28/22

* Presentation (5 points): 3/7/21 (Week 10)

* Product (40 points): Due 11:59:59 PM, 3/14/21


---
# Product

### Four components: 

* A web-deployed portfolio showcasing your [#dataviz](https://twitter.com/search?q=%23DataViz&amp;src=tyah)
  skills.
  + [distill](https://rstudio.github.io/distill/) (what I'll lecture on), 
    [R Markdown](https://bookdown.org/yihui/rmarkdown/rmarkdown-site.html), or
    [blogdown](https://bookdown.org/yihui/blogdown/) website
  + Technical document with [pagedown](https://github.com/rstudio/pagedown) or
    [bookdown](https://bookdown.org/yihui/bookdown/)
  + Scientific poster with [pagedown](https://github.com/rstudio/pagedown)
  + [flexdashboard](https://rmarkdown.rstudio.com/flexdashboard/)

---

* At least three finalized data displays, with each accompanied by a strong 
  narrative/story, as well as the history of how the visualization changed over 
  time.
  
* Housed on GitHub
  + Fully reproducible

* Deployed through [GitHub pages](https://pages.github.com) (or netlify or similar)


---
# Proposal

### Four components:

* Show me some evidence that you've at least played around with the course data and that you have some ideas of what you want to do

* *Very* preliminary visualizations, and/or hand-sketches of visuals you'd like to make, noting the data sources/columns to be used

* Identification of the intended audience for each viz 

* The intended message to be communicated for each viz 


--
## Main point - feedback!

---
# Draft

* Expected to still be a work in progress
  + Data visualizations should be largely complete

* Deployment not expected

* Provided to your peers so they can learn from you as much as you can learn from their feedback

---
# Peer Review
* We are all professionals here. It is imperative we act like it. 

* Understand the purpose of the exercise.  

* Zero tolerance policy for inappropriate comments

* Should be vigorously encouraging 

--

### Utilizing GitHub
You'll be assigned three proposals to review (3 points each, plus one bonus point for free)

* Fork their repo, embed comments &amp; suggest changes to their code, submit a PR

---
# Presentation 
Order randomly assigned. Basically a chance to share what you created!

* Presentation length will be determined later, but likely to be in the 10-15 minute range (note - you will present as a group)

* Share the final products

* Share the prior iterations

* Discuss the progression along the way and why specific changes were made

* What challenges did you face along the way? What victories did you have that you are particularly proud of?


---
class: inverse-blue middle
# Grading

---
# Points
### 150 points total
* 3 labs at 15 points each (45 points; 30%) 
* 1 homework assignments at 30 points each (20%)
* 1 Data Viz in the Wild (5 points; 3%)
* Final Project (70 points; 50%)
  + Proposal (5 points; 3%)
  + Draft (10 points; 7%)
  + Peer review (10 points; 7%)
  + Presentation (5 points; 3%)
  + Product (40 points; 27%)

---

# Grading

|  **Lower percent** |**Lower point range**  | **Grade** | **Upper point range**  | **Upper percent**|
|  :-------: | :-----------------| :-:| :---------------: | -----:|
|  0.970+    | (146 pts or more) | A+ |                   |       |
|  0.930     | (140 pts)         | A  | (145 pts)         | 0.969 |
|  0.900     | (135 pts)         | A- | (139 pts)         | 0.929 |
|  0.870     | (131 pts)         | B+ | (134 pts)         | 0.899 |
|  0.830     | (125 pts)         | B  | (130 pts)         | 0.869 |
|  0.800     | (120 pts)         | B- | (124 pts)         | 0.829 |
|  0.770     | (116 pts)         | C+ | (119 pts)         | 0.799 |
|  0.730     | (110 pts)         | C  | (115 pts)         | 0.769 |
|  0.700     | (105 pts)         | C- | (109 pts)         | 0.739 |
|            |                   | F  | (104 pts or less) | 0.699 |

---
class: inverse
background-image:url(https://d194ip2226q57d.cloudfront.net/original_images/10_Tips_for_Workplace_Communication)
background-size:contain


---
class: inverse-blue middle
# Data Visualization Competition

---
# Optional: opt-in/opt-out
* This term, we are hosting a data visualization competition hosted by USAFacts, who provided us with the course data

* This is completely optional and you should feel under no obligation to be part of the competition

* As a group, you neeed to decide whether to opt-in or opt-out by the start of class in Week 3

---
# A note on competition

* Sometimes competition can lead to toxic environments. Let's not do that.

* As previous portions of the syllabus should have made clear, this class is inherently collaborative **as a class** (i.e., you will be pointing out ways to improve your peers visuals through your peer review).

* Should not be stress-inducing. Intended to be a fun way to challenge yourself to do your best work.

---
# Competition
* Week 10, all student groups will present on their final projects.

* Those who opt-in  will provide their presentations to  three judges (one from UO, one from USAFacts, and one external to both organizations)

* Judges evaluate the visuals using a rating scale (which I will create) and note strengths/weaknesses

* Judges will make ratings independently initially, then  will confer to declare a winner

* You will receive the judges' feedback, in addition to mine (which all groups will receive at the end of the term)

---
# Competition

* Will be in a room besides this one

* I will developer a flier to advertise the competition and invite attendees

* Will be both live and virtual (i.e., there will be people joining via zoom)

---
# Conditions for participating

* Must opt-in

* Must build your visuals to match the USAFacts style guide

  + Some of this is a bit tricky/finnicky with ggplot2 theming. I would be happy to help you with this outside of class
  
---
# Why participate?

* The winning group will have their best visual, or possibly all of their visuals, featured on the USAFacts website

.pull-left[
* The winning group will also all get copies of [Alberto Cairo's](https://twitter.com/albertocairo) [the truthful art](https://www.bookdepository.com/Truthful-Art-Alberto-Cairo/9780321934079).
]

.pull-right[
![](https://d1w7fb2mkkr3kw.cloudfront.net/assets/images/book/lrg/9780/3219/9780321934079.jpg)
]

---
class: inverse-orange middle
# Questions?


---
class: inverse-green center bottom
background-image:url(https://cdn-images-1.medium.com/max/1600/1*aFHTAkhTkyWD93-UGRttPw.png)
background-size:contain

## Full lecture [here](https://www.youtube.com/watch?v=X7Cl3lwxXi4)

Please do watch the video and read the chapter.

---
# Quick pop quiz
Talk with your neighbor. What do these terms mean?

* stage
* commit
* push
* pull
* clone
* fork
* branch
* merge
* merge conflict
* pull request
* stash

---
class: inverse-blue middle
# Clone the course repo

## Why would we probably not want to fork the repo?

---
class: inverse-orange middle
# Course data

---
# Getting started
* To make it as easy as possible, I wrote a small R package to make accessing the data easier

* Install with 


```r
remotes::install_github("datalorax/edld652")
```


---
# Setting your key
* When you first load the package, you will see a message asking you to set a key.

* There is a document on canvas showing you how to do this. We'll go through it together now.

* You only need to do this once, then you can forget about it.

* Please do not share this key with others outside of this class - don't commit it to any repo.

* After you've set your key, go to **Session** on your menu and select **Restart R**.

---
# Check to see if all is working
After you've done everything on the prior slide, run the following to make sure it's working


```r
library(edld652)
list_datasets()
```

```
##  [1] "EDFacts_acgr_lea_2011_2019"                                
##  [2] "EDFacts_acgr_sch_2011_2019"                                
##  [3] "EDFacts_math_achievement_lea_2010_2019"                    
##  [4] "EDFacts_math_achievement_sch_2010_2019"                    
##  [5] "EDFacts_math_participation_lea_2013_2019"                  
##  [6] "EDFacts_math_participation_sch_2013_2019"                  
##  [7] "EDFacts_rla_achievement_lea_2010_2019"                     
##  [8] "EDFacts_rla_achievement_sch_2010_2019"                     
##  [9] "EDFacts_rla_participation_lea_2013_2019"                   
## [10] "EDFacts_rla_participation_sch_2013_2019"                   
## [11] "NCES_CCD_fiscal_district_2010"                             
## [12] "NCES_CCD_fiscal_district_2011"                             
## [13] "NCES_CCD_fiscal_district_2012"                             
## [14] "NCES_CCD_fiscal_district_2013"                             
## [15] "NCES_CCD_fiscal_district_2014"                             
## [16] "NCES_CCD_fiscal_district_2015"                             
## [17] "NCES_CCD_fiscal_district_2016"                             
## [18] "NCES_CCD_fiscal_district_2017"                             
## [19] "NCES_CCD_fiscal_district_2018"                             
## [20] "NCES_CCD_nonfiscal_district_2017_2021_directory"           
## [21] "NCES_CCD_nonfiscal_district_2017_2021_disabilities"        
## [22] "NCES_CCD_nonfiscal_district_2017_2021_english_learners"    
## [23] "NCES_CCD_nonfiscal_district_2017_2021_membership"          
## [24] "NCES_CCD_nonfiscal_district_2017_2021_staff"               
## [25] "NCES_CCD_nonfiscal_school_2017_2020_lunch_program"         
## [26] "NCES_CCD_nonfiscal_school_2017_2020_school_characteristics"
## [27] "NCES_CCD_nonfiscal_school_2017_2020_staff"                 
## [28] "NCES_CCD_nonfiscal_school_2017_2021_directory"             
## [29] "NCES_CCD_nonfiscal_school_2017_membership"                 
## [30] "NCES_CCD_nonfiscal_school_2018_membership"                 
## [31] "NCES_CCD_nonfiscal_school_2019_membership"                 
## [32] "NCES_CCD_nonfiscal_school_2020_membership"                 
## [33] "NCES_CCD_nonfiscal_state_2017_2020_directory"              
## [34] "NCES_CCD_nonfiscal_state_2017_2020_staff"                  
## [35] "NCES_CCD_nonfiscal_state_2017_2021_membership"
```

---
# Accessing a dataset

* The `list_datasets()` function shows you a list of all available datasets

* You can import any of these into R with the `get_data()` function by passing the name of the dataset as a string.

For example: Average cohort graduate rates for local education agency data, 2011 to 2019


```r
acgd &lt;- get_data("EDFacts_acgr_lea_2011_2019")
```

```
## 
  |                                             
  |                                       |   0%
  |                                             
  |                                       |   1%
  |                                             
  |=                                      |   2%
  |                                             
  |=                                      |   3%
  |                                             
  |==                                     |   4%
  |                                             
  |==                                     |   5%
  |                                             
  |==                                     |   6%
  |                                             
  |===                                    |   7%
  |                                             
  |===                                    |   8%
  |                                             
  |====                                   |   9%
  |                                             
  |====                                   |  10%
  |                                             
  |====                                   |  11%
  |                                             
  |=====                                  |  12%
  |                                             
  |=====                                  |  13%
  |                                             
  |=====                                  |  14%
  |                                             
  |======                                 |  14%
  |                                             
  |======                                 |  15%
  |                                             
  |======                                 |  16%
  |                                             
  |=======                                |  17%
  |                                             
  |=======                                |  18%
  |                                             
  |=======                                |  19%
  |                                             
  |========                               |  19%
  |                                             
  |========                               |  20%
  |                                             
  |========                               |  21%
  |                                             
  |========                               |  22%
  |                                             
  |=========                              |  22%
  |                                             
  |=========                              |  23%
  |                                             
  |=========                              |  24%
  |                                             
  |==========                             |  25%
  |                                             
  |==========                             |  26%
  |                                             
  |==========                             |  27%
  |                                             
  |===========                            |  27%
  |                                             
  |===========                            |  28%
  |                                             
  |===========                            |  29%
  |                                             
  |============                           |  30%
  |                                             
  |============                           |  31%
  |                                             
  |============                           |  32%
  |                                             
  |=============                          |  32%
  |                                             
  |=============                          |  33%
  |                                             
  |=============                          |  34%
  |                                             
  |=============                          |  35%
  |                                             
  |==============                         |  35%
  |                                             
  |==============                         |  36%
  |                                             
  |==============                         |  37%
  |                                             
  |===============                        |  37%
  |                                             
  |===============                        |  38%
  |                                             
  |===============                        |  39%
  |                                             
  |===============                        |  40%
  |                                             
  |================                       |  40%
  |                                             
  |================                       |  41%
  |                                             
  |================                       |  42%
  |                                             
  |=================                      |  42%
  |                                             
  |=================                      |  43%
  |                                             
  |=================                      |  44%
  |                                             
  |=================                      |  45%
  |                                             
  |==================                     |  45%
  |                                             
  |==================                     |  46%
  |                                             
  |==================                     |  47%
  |                                             
  |===================                    |  48%
  |                                             
  |===================                    |  49%
  |                                             
  |===================                    |  50%
  |                                             
  |====================                   |  50%
  |                                             
  |====================                   |  51%
  |                                             
  |====================                   |  52%
  |                                             
  |=====================                  |  53%
  |                                             
  |=====================                  |  54%
  |                                             
  |=====================                  |  55%
  |                                             
  |======================                 |  56%
  |                                             
  |======================                 |  57%
  |                                             
  |=======================                |  58%
  |                                             
  |=======================                |  59%
  |                                             
  |=======================                |  60%
  |                                             
  |========================               |  61%
  |                                             
  |========================               |  62%
  |                                             
  |=========================              |  63%
  |                                             
  |=========================              |  64%
  |                                             
  |=========================              |  65%
  |                                             
  |==========================             |  66%
  |                                             
  |==========================             |  67%
  |                                             
  |===========================            |  68%
  |                                             
  |===========================            |  69%
  |                                             
  |===========================            |  70%
  |                                             
  |============================           |  71%
  |                                             
  |============================           |  72%
  |                                             
  |============================           |  73%
  |                                             
  |=============================          |  73%
  |                                             
  |=============================          |  74%
  |                                             
  |=============================          |  75%
  |                                             
  |==============================         |  76%
  |                                             
  |==============================         |  77%
  |                                             
  |==============================         |  78%
  |                                             
  |===============================        |  78%
  |                                             
  |===============================        |  79%
  |                                             
  |===============================        |  80%
  |                                             
  |===============================        |  81%
  |                                             
  |================================       |  81%
  |                                             
  |================================       |  82%
  |                                             
  |================================       |  83%
  |                                             
  |=================================      |  83%
  |                                             
  |=================================      |  84%
  |                                             
  |=================================      |  85%
  |                                             
  |=================================      |  86%
  |                                             
  |==================================     |  86%
  |                                             
  |==================================     |  87%
  |                                             
  |==================================     |  88%
  |                                             
  |===================================    |  89%
  |                                             
  |===================================    |  90%
  |                                             
  |===================================    |  91%
  |                                             
  |====================================   |  91%
  |                                             
  |====================================   |  92%
  |                                             
  |====================================   |  93%
  |                                             
  |====================================   |  94%
  |                                             
  |=====================================  |  94%
  |                                             
  |=====================================  |  95%
  |                                             
  |=====================================  |  96%
  |                                             
  |====================================== |  96%
  |                                             
  |====================================== |  97%
  |                                             
  |====================================== |  98%
  |                                             
  |====================================== |  99%
  |                                             
  |=======================================|  99%
  |                                             
  |=======================================| 100%
```

```r
acgd
```

```
## # A tibble: 11,326 × 29
##    ALL_COHORT ALL_RATE CWD_COHORT CWD_RATE
##         &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;   
##  1        252 80                3 PS      
##  2        398 75               47 70-79   
##  3       1020 89               51 40-49   
##  4        750 91               35 60-69   
##  5        128 55-59            15 LT50    
##  6        166 90-94             9 GE50    
##  7        336 90               30 60-79   
##  8        273 77               11 LT50    
##  9        134 70-74             4 PS      
## 10        266 58               33 50-59   
## # … with 11,316 more rows, and 25 more
## #   variables: DATE_CUR &lt;chr&gt;, ECD_COHORT &lt;dbl&gt;,
## #   ECD_RATE &lt;chr&gt;, FIPST &lt;chr&gt;, FILEURL &lt;chr&gt;,
## #   LEAID &lt;chr&gt;, LEANM &lt;chr&gt;, LEP_COHORT &lt;dbl&gt;,
## #   LEP_RATE &lt;chr&gt;, MAM_COHORT &lt;dbl&gt;,
## #   MAM_RATE &lt;chr&gt;, MAS_COHORT &lt;dbl&gt;,
## #   MAS_RATE &lt;chr&gt;, MBL_COHORT &lt;dbl&gt;, …
```

---

```r
acgd
```

```
## # A tibble: 11,326 × 29
##    ALL_COHORT ALL_RATE CWD_COHORT CWD_RATE
##         &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;   
##  1        252 80                3 PS      
##  2        398 75               47 70-79   
##  3       1020 89               51 40-49   
##  4        750 91               35 60-69   
##  5        128 55-59            15 LT50    
##  6        166 90-94             9 GE50    
##  7        336 90               30 60-79   
##  8        273 77               11 LT50    
##  9        134 70-74             4 PS      
## 10        266 58               33 50-59   
## # … with 11,316 more rows, and 25 more
## #   variables: DATE_CUR &lt;chr&gt;, ECD_COHORT &lt;dbl&gt;,
## #   ECD_RATE &lt;chr&gt;, FIPST &lt;chr&gt;, FILEURL &lt;chr&gt;,
## #   LEAID &lt;chr&gt;, LEANM &lt;chr&gt;, LEP_COHORT &lt;dbl&gt;,
## #   LEP_RATE &lt;chr&gt;, MAM_COHORT &lt;dbl&gt;,
## #   MAM_RATE &lt;chr&gt;, MAS_COHORT &lt;dbl&gt;,
## #   MAS_RATE &lt;chr&gt;, MBL_COHORT &lt;dbl&gt;, …
```

---
# Accessing documentation
* The names of the datasets themselves can sometimes be a bit cryptic

* The variable names are often not interpretable at all (particularly the financial data)

* You can access the documentation for any dataset with the `get_documentation()` function, again passing the name of the dataset

* This function operates slightly differently on Mac/Windows

---
* Mac
  + Creates a folder in your current working directory called `data-documentation`
  + Downloads the documentation and places it in that folder
  + Opens the documentation
  + If the same documentation is requested again, skip the download and just open
* Windows
  + Prints a link to your console where documentation can be downloaded
  

--
Note - if any Windows users want to let me borrow their computer for a bit after class one day, I might be able to get it working for Windows as well.
  
---
class: inverse-blue middle

# Data demo

For the next 30 minutes or so we will: 

* Walk through the [overview of the course data](../2021-12-10-accessing-the-data/) together, and then 

* Work in small groups to continue to explore the data and come up with new visualizations on your own.

---
class: inverse-red middle

# Intro to textual data

---
# Structured vs unstructured

* Most every dataset you've ever worked with is what is referred to as a **structured** dataset - it has rows and columns.

* But there is an incredible amount of data out there that is **unstructured** - it just sort of exists


--
* Most text data is unstructured. How would you analyze the contents of a book? No rows or columns there

---
# Getting text data

There are **many** ways to get text data. Any digital text could potentially be used as textual data.


--
How about Wikipedia?


--
Anything that lives on the web is a common use case. Social media data being perhaps primary among them.

---
# "Screen" scraping
Short foray into web scraping. It's not expected you fully follow this. More about "exposure" and less about building competencies.

--

Use the [rvest](https://rvest.tidyverse.org/) package to scrape the data you see "on the screen".


--

Let's read in the Wikipedia page on Eugene


```r
library(rvest)
eugene &lt;- read_html("https://en.wikipedia.org/wiki/Eugene%2C_Oregon")
```


---
# Grab paragraphs
The `"#mw-content-text &gt; div.mw-parser-output &gt; p"` is the CSS selector that I pulled from the website

```r
paragraphs &lt;- eugene %&gt;% 
  html_elements("#mw-content-text &gt; div.mw-parser-output &gt; p") %&gt;%
  html_text2()
```

The first paragraph is just an empty line, so they are numbered p + 1

Print the first paragraph


```r
cat(stringr::str_wrap(paragraphs[2], 50))
```

```
## Eugene (/juːˈdʒiːn/ yoo-JEEN) is a city in the
## U.S. state of Oregon, in the Pacific Northwest. It
## is at the southern end of the Willamette Valley,
## near the confluence of the McKenzie and Willamette
## rivers, about 50 miles (80 km) east of the Oregon
## Coast.[7]
```

---
# Print the fourth paragraph


```r
cat(stringr::str_wrap(paragraphs[5], 50))
```

```
## The first people to settle in the Eugene area were
## known as the Kalapuyans, also written Calapooia
## or Calapooya. They made "seasonal rounds," moving
## around the countryside to collect and preserve
## local foods, including acorns, the bulbs of the
## wapato and camas plants, and berries. They stored
## these foods in their permanent winter village.
## When crop activities waned, they returned to their
## winter villages and took up hunting, fishing, and
## trading.[19][20] They were known as the Chifin
## Kalapuyans and called the Eugene area where they
## lived "Chifin", sometimes recorded as "Chafin" or
## "Chiffin".[21][22]
```

---
# Analysis
How do we analyze the text? What we we even analyze?


--
First, let's structure it! Turn the text into a simple data frame.

--

```r
library(tidyverse)
eugene_df &lt;- tibble(
  paragraph = seq_along(paragraphs),
  description = paragraphs
)
eugene_df
```

```
## # A tibble: 149 × 2
##    paragraph
##        &lt;int&gt;
##  1         1
##  2         2
##  3         3
##  4         4
##  5         5
##  6         6
##  7         7
##  8         8
##  9         9
## 10        10
## # … with 139 more rows, and 1 more variable:
## #   description &lt;chr&gt;
```

---
# Can we analyze it now?

Not really... what would we analyze?


--
Words!

Let's break it into words. This is where the [tidytext](https://juliasilge.github.io/tidytext/) package comes into play.

---
# The `unnest_tokens()` function

Just like most functions in the tidyverse, we pipe our data to `unnest_tokens()`

* First argument is the name of the new column we want in our data

* Second argument is the text data to process

* Third argument is how the text should processed. The default is `"words"`, meaning the text will be broken into words.

---
# Example


```r
library(tidytext)
eugene_tidy_words &lt;- eugene_df %&gt;% 
  unnest_tokens(word, description)
eugene_tidy_words
```

```
## # A tibble: 9,621 × 2
##    paragraph word     
##        &lt;int&gt; &lt;chr&gt;    
##  1         2 eugene   
##  2         2 juːˈdʒiːn
##  3         2 yoo      
##  4         2 jeen     
##  5         2 is       
##  6         2 a        
##  7         2 city     
##  8         2 in       
##  9         2 the      
## 10         2 u.s      
## # … with 9,611 more rows
```

Not perfect, but pretty good

---
# What to do now?
Let's count some words!


```r
eugene_tidy_words %&gt;% 
  count(word, sort = TRUE)
```

```
## # A tibble: 2,874 × 2
##    word       n
##    &lt;chr&gt;  &lt;int&gt;
##  1 the      730
##  2 and      299
##  3 of       293
##  4 in       287
##  5 eugene   204
##  6 a        180
##  7 to       159
##  8 is        94
##  9 for       90
## 10 was       87
## # … with 2,864 more rows
```

---
# Plot the top 15 words


```r
eugene_tidy_words %&gt;% 
  count(word, sort = TRUE) %&gt;% 
  mutate(word = reorder(word, n)) %&gt;% # make y-axis ordered by n
  slice(1:15) %&gt;% # select only the first 15 rows
  ggplot(aes(n, word)) +
    geom_col(fill = "cornflowerblue")
```

![](w1_files/figure-html/unnamed-chunk-17-1.png)&lt;!-- --&gt;

---
# Not very informative

## Why?

--
Most of the words are common words like "the", "and", "of" (top three words)


--
These are referred to as "stop words".


--
Luckily, **tidytext** provides us with a dictionary of stop words. We can use an `anti_join()` with this dictionary to remove these words.

---
# Quick refresher

A `semi_join()` works just like an `inner_join()`, but without adding any columns. A `semi_join()` works by **keeping** only rows that are in common with the two datasets.


--
An `anti_join()` does basically the opposite, by **removing** any rows that are in common between the two datasets.

---
# Look at the stop words
This dataset is available to you as soon as you load **tidytext**. 

There are three lexicons - I usually use all three.


```r
stop_words 
```

```
## # A tibble: 1,149 × 2
##    word        lexicon
##    &lt;chr&gt;       &lt;chr&gt;  
##  1 a           SMART  
##  2 a's         SMART  
##  3 able        SMART  
##  4 about       SMART  
##  5 above       SMART  
##  6 according   SMART  
##  7 accordingly SMART  
##  8 across      SMART  
##  9 actually    SMART  
## 10 after       SMART  
## # … with 1,139 more rows
```

---
# Count
Let's try counting again without the stop words included.

.pull-left[


```r
eugene_tidy_words %&gt;% 
* anti_join(stop_words) %&gt;%
  count(word, sort = TRUE)
```

```
## # A tibble: 2,580 × 2
##    word           n
##    &lt;chr&gt;      &lt;int&gt;
##  1 eugene       204
##  2 city          62
##  3 oregon        58
##  4 university    50
##  5 community     29
##  6 eugene's      27
##  7 lane          24
##  8 center        21
##  9 home          21
## 10 campus        20
## # … with 2,570 more rows
```

]

.pull-right[
## So much more informative!
]

---
# Plot the top 15 words


```r
eugene_tidy_words %&gt;% 
  anti_join(stop_words) %&gt;% 
  count(word, sort = TRUE) %&gt;% 
  mutate(word = reorder(word, n)) %&gt;% # make y-axis ordered by n
  slice(1:15) %&gt;% # select only the first 15 rows
  ggplot(aes(n, word)) +
    geom_col(fill = "cornflowerblue")
```

![](w1_files/figure-html/unnamed-chunk-20-1.png)&lt;!-- --&gt;

---
# Add the headers in 
I hid the code here because it's weird and inefficient but the HTML structure made it difficult. You can look at the source if you want. The new dataframe is called `eugene_tidy_words2`.


```
## # A tibble: 149 × 3
##    paragraph header 
##        &lt;int&gt; &lt;chr&gt;  
##  1         1 Intro  
##  2         2 Intro  
##  3         3 Intro  
##  4         4 History
##  5         5 History
##  6         6 History
##  7         7 History
##  8         8 History
##  9         9 History
## 10        10 History
## # … with 139 more rows, and 1 more variable:
## #   description &lt;chr&gt;
```

---
# Count words by header

Not surprisingly, "eugene" appears to be the most common among multiple categories.

We might want to remove "eugene" as well.


```r
eugene_tidy_words2 %&gt;% 
  unnest_tokens(word, description) %&gt;% 
  count(header, word, sort = TRUE) %&gt;% 
  anti_join(stop_words)
```

```
## # A tibble: 3,584 × 3
##    header           word           n
##    &lt;chr&gt;            &lt;chr&gt;      &lt;int&gt;
##  1 Arts and culture eugene        54
##  2 History          eugene        49
##  3 Infrastructure   eugene        20
##  4 History          university    19
##  5 Arts and culture church        17
##  6 History          city          17
##  7 Education        eugene        15
##  8 Education        school        14
##  9 History          police        14
## 10 History          protest       14
## # … with 3,574 more rows
```

---
# Plot
Top 15 words by header


```r
p &lt;- eugene_tidy_words2 %&gt;% 
  unnest_tokens(word, description) %&gt;% 
  count(header, word, sort = TRUE) %&gt;% 
  anti_join(stop_words) %&gt;% 
  group_by(header) %&gt;% 
  slice(1:15) %&gt;% 
  ggplot(aes(n, word)) +
    geom_col(fill = "cornflowerblue") +
    facet_wrap(~header, scales = "free_y") 
```

---
class: full-size-fig

&lt;img src="w1_files/figure-html/unnamed-chunk-24-1.png" width="90%" /&gt;

---
class: inverse-green middle
# Next time

* Finish up on text data
* Discuss string manipulations
* Discuss distribution/binning
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="https://platform.twitter.com/widgets.js"></script>
<script>var slideshow = remark.create({
"navigation": {
"scroll": false
},
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
