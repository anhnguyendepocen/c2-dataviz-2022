---
title: Course data
author: Daniel Anderson
date: '2021-12-10'
assigned: '2021-02-24'
due: '2021-03-03'
slug: accessing-the-data
toc: true
categories:
  - Data
  - Data Visualization Competition
tags:
  - Data
  - Final
---

```{r include = FALSE}
knitr::opts_chunk$set(
  message = FALSE
)
```

The data we will be using for this course comes from USAFacts, who collated the data from various other sources. These data are not particularly granular. The smallest unit of analysis is schools, so you will have to think on a macro level about the types of questions you want to address. Full instructions to accessing the data are provided for you on canvas in the Files section (see [here](https://canvas.uoregon.edu/courses/192573/files?preview=12782218)). 

The purpose of this document is to walk you through a few files and discuss some of their properties. Let's start by looking at the available files.

```{r }
library(edld652)
list_datasets()
```

The names of these are not always super clear, and the variable names within the files are also regularly not straightforward. To start, anything that says `lea` is at the school district level, and anything that says `sch` or `school` is at the school level. 

## Data documentation
You can import any of these datasets and start to play around, but is **highly** encouraged that you also look at the corresponding documentation. Let's actually start by looking at the documentation for `"EDFacts_acgr_sch_2011_2019"`, which we know is at the school level, but otherwise it's pretty unclear.

```{r eval = FALSE}
get_documentation("EDFacts_acgr_sch_2011_2019")
```

If you're on a Mac, the above code will download the documentation for this file and put it in a `data-documenation` directory (folder) inside your current working directory (your R project) and automatically open the file (in Microsoft Word in this case, but some documentation is also in Excel). Unfortunately, Windows machines make this process more difficult, so if you're on a Windows machine it will instead just print a link to the console that you can paste into your browser to get the documentation.

Scanning through this documentation we see that this file contains data (at the school level, given the file name) on the four-year adjusted-cohort graduation rates. According to the documentation, this is defined as

> The four-year adjusted-cohort graduation rate is the number of students who graduate in four years with a regular high school diploma divided by the number of students who formed the cohort for that graduating class. The four-year adjusted cohort rate also includes students who graduate in less than four years.

Section 1.6 is also important. It describes steps taken to ensure that individual students cannot be identified in the directory. This entails two steps:

* Cells with 1-5 students are suppressed. These are coded `PS`.
* Medium sized groups are binned (e.g. `"< 20%"`). The binning method depends on the size of the group. See Table 2 for more details.

## Getting the data
Let's go ahead and download the school district level version of this file, rather than schools, so it will be a bit quicker. Note that some of these datasets will take several minutes to download/import.

```{r }
grad_rates <- get_data("EDFacts_acgr_lea_2011_2019")
grad_rates
```

Tables 6 and 7 in the documentation describes the variable naming conventions used here. However, there are a few variables in this file that are not described in the documentation, most notably `"FIPST"`, `"LEAID"`, and `"LEANM"`. These are going to be critical for joining other sources of data. The `"FIPST"` identifies the state, while the latter two identify the specific school district.

### Adding in state information
There are many places where you can get a file linking the `"FIPST"` with the actual name. For example, there's a file from Kieran Healy available [here](https://github.com/kjhealy/fips-codes/blob/master/state_fips_master.csv). Let's actually use that file to merge in information about the states.

```{r }
state_info <- readr::read_csv("https://github.com/kjhealy/fips-codes/raw/master/state_fips_master.csv")
state_info
```

Note in the above I'm using [readr](https://readr.tidyverse.org/) to read in the CSV, and I've changed the link slight so it says `"raw"` instead of `"blob"`. This is a general method you can use for reading in any CSV from GitHub.

Note that the FIPS is coded slightly differently in this file - it's an integer So to join these files we should first change `"FIPST"` so it is also an integer

```{r }
library(dplyr) # could also just load the entire tidyverse
grad_rates <- grad_rates %>% 
  mutate(fips = readr::parse_number(FIPST))
```

And now we can join them.

```{r message = TRUE}
grad_rates <- left_join(grad_rates, state_info)
grad_rates %>% 
  select(state_name, ALL_RATE)
```

## Joining other files
Let's say we're interested in visualizing how the total revenue for a district relates to 

proportion of students classified as economically disadvantaged relates to the economically disadvantaged cohort graduation rate. For this, we would need to join in the lunch program dataset. Note that this takes several minutes to import (on my computer with my home internet connection).

```{r }
fiscal_18 <- get_data("NCES_CCD_fiscal_district_2018")

# select only the variables we want
fiscal_18 <- fiscal_18 %>% 
  select(FIPST, LEAID, TOTALREV)
fiscal_18
```

Note that there are some instances where the total revenue appears to be suppressed. This is not covered in the documentation, so I'm unsure of what these negative values mean, but I can find out if there are groups interested in using this data. I'm going to go ahead and remove these from the file before the join. Then we'll join the file with our `grad_rates` file and create a few plots.

```{r }
grad_rates <- left_join(grad_rates, filter(fiscal_18, TOTALREV > 0))
grad_rates
```

Let's create an overall scatterplot to start.

```{r }
library(ggplot2)
theme_set(theme_minimal(15))

ggplot(grad_rates, aes(TOTALREV, ALL_RATE)) +
  geom_point()
```

Wow! That is really not helpful! The problem here is the binning of the Cohort Graduation rate. Let's transform this to an approximately numeric scale by putting the rate in the middle of the range, if there is a range, and otherwise taking the exact percent. 

```{r }
grad_rates %>% 
  count(ALL_RATE)
```

Above is a small section of all the rates. Let's split this column based on the `"-"` character. There are many ways to do this, but probably the most straightforward way with the tidyverse would be

```{r }
grad_rates %>% 
  count(ALL_RATE) %>% 
  tidyr::separate(ALL_RATE, c("lower", "upper"), sep = "-")
```

We get a warning because there are many cases where the range is not binned.  Let's now drop any cases that cannot easily be transformed to numeric (e.g., `"GE50"`, `"PS"`), fill in the missing values with the `lower` value, then calculate the mean. Note when I calculate the mean, I have to do so `rowwise()`.

```{r }
grad_rates %>% 
  count(ALL_RATE) %>% 
  tidyr::separate(ALL_RATE, c("lower", "upper"), sep = "-") %>% 
  filter(!grepl("G|L|P", lower)) %>% 
  mutate(
    upper = ifelse(is.na(upper), lower, upper),
    lower = as.numeric(lower),
    upper = as.numeric(upper)
  ) %>% 
  rowwise() %>% 
  mutate(mean = mean(c(lower, upper))) %>% 
  ungroup()
```

Perfect! Note - if you don't fully follow all of the above right now, that's okay. It's a bit complicated. It's also not done. We were using `count()` to basically verify that it wa all working, but now we need to do the same thing to our actual dataframe.

```{r }
grad_rates <- grad_rates %>% 
  tidyr::separate(ALL_RATE, c("lower", "upper"), sep = "-") %>% 
  filter(!grepl("G|L|P", lower)) %>% 
  mutate(
    upper = ifelse(is.na(upper), lower, upper),
    lower = as.numeric(lower),
    upper = as.numeric(upper)
  ) %>% 
  rowwise() %>% 
  mutate(mean = mean(c(lower, upper))) %>% 
  ungroup()
```

And now we can try our original plot again.

```{r }
ggplot(grad_rates, aes(TOTALREV, mean)) +
  geom_point()
```

Note that we have a lot of data that is bunched up near the bottom. Let's try a log transformation of the x-axis.

```{r }
ggplot(grad_rates, aes(TOTALREV, mean)) +
  geom_point() +
  scale_x_log10()
```

This is a bit more helpful, but let's make the x-axis more interpretable.

```{r }
ggplot(grad_rates, aes(TOTALREV, mean)) +
  geom_point() +
  scale_x_log10(labels = scales::dollar)
```

Better.Let's go ahead and finish styling the plot a bit to make it even more clear.

```{r }
ggplot(grad_rates, aes(TOTALREV, mean)) +
  geom_point(
    stroke = 0,
    alpha = 0.3,
    color = "gray20",
    size = 2
  ) +
  scale_x_log10("Total District Revenue", labels = scales::dollar) +
  labs(
    y = "Approximate average graduation rate",
    title = stringr::str_wrap("School districts with higher total revenue generally have higher graduation rates", 60),
    subtitle = "Each point represents a school district",
    caption = "Note the log transformation of the x-axis"
  ) +
  theme(plot.title.position = "plot")
```

One last example, maybe we want to see this relation by state. We could try something like `facet_wrap()` but that is likely to be unmanageably large and take a lot of mental processing to track across each state. Instead, we can use the [geofacet](https://hafen.github.io/geofacet/) package to arrange the facets according to the basic geography of the US. For example

```{r fig.height = 10, fig.width = 10}
library(geofacet)

grad_rates %>% 
  tidyr::drop_na(state_abbr) %>% 
ggplot(aes(TOTALREV, mean)) +
  geom_point(
    stroke = 0,
    alpha = 0.3,
    color = "gray20",
    size = 2
  ) +
  facet_geo(~state_abbr) +
  scale_x_log10("Total District Revenue", labels = scales::dollar) +
  labs(
    y = "Approximate average graduation rate",
    title = stringr::str_wrap("School districts with higher total revenue generally have higher graduation rates", 60),
    subtitle = "Each point represents a school district",
    caption = "Note the log transformation of the x-axis"
  ) +
  theme(
    plot.title.position = "plot",
    axis.text.x = element_blank()
  )
```

This was obviously very quick, and I omitted the x-axis labels entirely (not a good idea), but gives you a basic idea of some of what you can do. Note that the best visual will often depend on the medium, which is why we cover both in this class, and this is an example of a good visual but not for this medium, because the margins of the website end up squishing the figure too much. Later in the course, we'll learn about [distill](https://rstudio.github.io/distill/) which has some nice options for handling this sort of thing.
